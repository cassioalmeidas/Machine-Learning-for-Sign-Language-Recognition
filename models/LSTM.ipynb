{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMV3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPGVnHxwCr1u"
      },
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "python_random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTc8Ix8ZDF_T"
      },
      "source": [
        "# Import sentences\n",
        "sentences = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/LSTM/sentences.csv\", header = None).iloc[:,0]\n",
        "sentences = list(map(lambda x: x.split(\" \"),sentences))\n",
        "# Import labels\n",
        "labels = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/LSTM/labels2.csv\", header = None).iloc[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cklnvfevt5U-"
      },
      "source": [
        "# Transform each word in the generated sentences into their label value [1-276]\n",
        "word_labels = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/LSTM/sign_labels.csv\", header = None).values\n",
        "sentence_index = []\n",
        "for sentence in sentences:\n",
        "    sentence_index.append(list(map(lambda x: np.where(word_labels==x)[0][0],sentence)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw5xqLk1DRCc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "X = pad_sequences(\n",
        "    sentence_index, maxlen= 4, padding=\"post\", truncating=\"post\"\n",
        ")\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels.ravel())\n",
        "encoded_Y = encoder.transform(labels.ravel())\n",
        "y = to_categorical(encoded_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDsKC5VKDWxz"
      },
      "source": [
        "# Split the dataset into training and testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, test_size=0.2, shuffle= True, random_state = 42)\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "X_test = tf.convert_to_tensor(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNsjZst9DfyU"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Activation\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Define the model \n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(276, 32, input_length=4))\n",
        "  model.add(LSTM(512, dropout=0.1))\n",
        "  model.add(Dense(2421))\n",
        "  model.add(Activation('softmax'))\n",
        "  optimizer = optimizers.Adam(learning_rate=3e-4)\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9gqNUhNVR9T"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Fitting and prediction\n",
        "estimator = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=1)\n",
        "history = estimator.fit(\n",
        "    X_train, y_train, validation_data=(X_test, y_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ddsZGypV94m"
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv = cross_val_score(estimator,X,y,cv=kfold)\n",
        "print(cv.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVZGgrPD8Z2U"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}