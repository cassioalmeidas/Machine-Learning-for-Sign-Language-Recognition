{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvLSTM2D",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDLeinU4c5yy",
        "outputId": "4be47bac-d17f-4070-c273-a881ce3e5c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S94dlKGui3P"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as python_random\n",
        "import tensorflow as tf\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "python_random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Wwd9Zl6niL"
      },
      "source": [
        "#Import CSI values\n",
        "dataset = loadmat('/content/drive/My Drive/Colab Notebooks/dataset/dataset_lab_276_dl.mat')\n",
        "csi = dataset['csid_lab']\n",
        "csi_abs = np.abs(csi)\n",
        "csi_ang = np.angle(csi)\n",
        "# Concatenate and reshape\n",
        "csi_tensor = np.concatenate((csi_abs,csi_ang),1)\n",
        "csi_tensor = np.swapaxes(csi_tensor,0,3)\n",
        "csi_tensor = np.swapaxes(csi_tensor,1,3)\n",
        "csi_tensor = np.swapaxes(csi_tensor,2,3)\n",
        "del dataset,csi,csi_abs, csi_ang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUK739SSutZu"
      },
      "source": [
        "# Group the available words by their CSI values (20instances/word)\n",
        "words_csi = []\n",
        "for index in range(276):\n",
        "    round = 0\n",
        "    words_csi.append(csi_tensor[index])\n",
        "    for instance in range(19):\n",
        "        round += 276\n",
        "        words_csi[index] = np.concatenate((words_csi[index],csi_tensor[index+round]))\n",
        "    words_csi[index] = np.reshape(words_csi[index],(20,200,60,3))\n",
        "del csi_tensor,index,round,instance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDWH8PUtuwG2"
      },
      "source": [
        "# Import sentences\n",
        "data_sent = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ConvLSTM/sentences.csv\", header = None)\n",
        "sentences = data_sent.iloc[:,0]\n",
        "sentences = list(map(lambda x: x.split(\" \"), sentences))\n",
        "word_labels = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ConvLSTM/sign_labels.csv\", header = None).values\n",
        "# Transform each word in the generated sentences into their label value [1-276]\n",
        "sentence_index = []\n",
        "for sentence in sentences:\n",
        "    sentence_index.append(list(map(lambda x: np.where(word_labels==x)[0][0],sentence)))\n",
        "del data_sent, sentence, sentences,word_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr_IHCezux_M"
      },
      "source": [
        "# Keep only the sentences with 3 words\n",
        "# For Technical reasons we can't take the whole data\n",
        "sentence_index = sentence_index[:896]\n",
        "# Transform the sentences by their CSI values\n",
        "sentences_csi = []\n",
        "for sentence in sentence_index:\n",
        "    for instance in range(20):\n",
        "        sentence_temp = []\n",
        "        for word in sentence:\n",
        "            sentence_temp.append(words_csi[word][instance])\n",
        "        sentences_csi.append(np.concatenate(sentence_temp).reshape((-1,200,60,3)))\n",
        "del instance, sentence, sentence_index, sentence_temp, word, words_csi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMRgh9S9u0PK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Make the sequences all have the same length i.e. 3\n",
        "X = pad_sequences(\n",
        "    sentences_csi, maxlen=3, padding=\"post\", truncating=\"post\"\n",
        ")\n",
        "# shape(X) = (17920, 3, 200, 60, 3)\n",
        "del sentences_csi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlRR0WKgu1-V"
      },
      "source": [
        "# Import the 896 first labels\n",
        "sentence_labels = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ConvLSTM/labels.csv\", header = None).values.tolist()[:896]\n",
        "# Multiply each label 20 times to match the input.\n",
        "sent_labels = []\n",
        "for sentence in sentence_labels:\n",
        "    for instance in range(20):\n",
        "        sent_labels.append(sentence)\n",
        "del instance, sentence, sentence_labels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zoiIk37u33H"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Encode the variables\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(sent_labels)\n",
        "encoded_Y = encoder.transform(sent_labels)\n",
        "y = to_categorical(encoded_Y)\n",
        "del encoded_Y, encoder, sent_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7WXRdmHu6Ej"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, Activation, Flatten, Dense, AveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "# create model\n",
        "def baseline_model():\n",
        "    model = Sequential()\n",
        "    model.add(ConvLSTM2D(8, (3,3),\n",
        "                     input_shape=(3,200,60,3),\n",
        "                     activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(180))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer= 'SGD',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "estimator = KerasClassifier(build_fn=baseline_model, epochs=10,batch_size= 10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSGHkw1su97r"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, test_size=0.2, random_state=42)\n",
        "del X,y\n",
        "history = estimator.fit(X_train,y_train,validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed178ESuY3rh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for \n",
        "plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiElHOgn7ilB"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "kfold = KFold(n_splits = 5, shuffle=True, random_state=42)\n",
        "crossval = cross_val_score(estimator, X_train, y_train, cv = kfold)\n",
        "print(crossval.mean())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}